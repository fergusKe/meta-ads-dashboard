"""
A/B æ¸¬è©¦è¨­è¨ˆ Agent (Pydantic AI)

åŠŸèƒ½ï¼š
- åˆ†æžç¾æœ‰æ•¸æ“šæŽ¨å°Ž A/B æ¸¬è©¦å‡è¨­
- ç”¢å‡ºæ¸¬è©¦è®Šå› ã€æˆåŠŸæŒ‡æ¨™ã€æ¨£æœ¬èˆ‡æ™‚ç¨‹å»ºè­°
- æä¾›åŸ·è¡Œæª¢æŸ¥æ¸…å–®èˆ‡é€²éšŽç­–ç•¥
- å¯æ•´åˆ RAG æ¡ˆä¾‹æä¾›éˆæ„Ÿ

ç‰¹è‰²ï¼š
- å®Œå…¨åž‹åˆ¥å®‰å…¨è¼¸å‡º
- å·¥å…·æ•´ç†åŸºç¤ŽæŒ‡æ¨™ï¼ŒLLM è² è²¬ç­–ç•¥åŒ–å…§å®¹
"""

from __future__ import annotations

import os
from dataclasses import dataclass
from typing import Optional

import pandas as pd
from pydantic import BaseModel, Field
from pydantic_ai import Agent, RunContext


class MetricSnapshot(BaseModel):
    name: str
    value: float


class TestVariation(BaseModel):
    name: str = Field(description="è®Šé«”åç¨±æˆ–æè¿°")
    details: str = Field(description="è®Šé«”å…§å®¹")


class TestIdea(BaseModel):
    priority: str = Field(description="å„ªå…ˆç´šï¼ˆðŸ”´/ðŸŸ¡/ðŸŸ¢ï¼‰")
    variable: str = Field(description="æ¸¬è©¦è®Šå› ")
    hypothesis: str = Field(description="å‡è¨­")
    success_metrics: list[str] = Field(description="æˆåŠŸæŒ‡æ¨™", default_factory=list)
    guardrail_metrics: list[str] = Field(description="è­·æ¬„æŒ‡æ¨™", default_factory=list)
    test_duration: str = Field(description="å»ºè­°æ¸¬è©¦æœŸ")
    budget_allocation: str = Field(description="é ç®—æˆ–æµé‡é…ç½®")
    expected_impact: str = Field(description="é æœŸæˆæ•ˆ")
    variations: list[TestVariation] = Field(description="è®Šé«”è¨­è¨ˆ", default_factory=list)


class ABExecutionChecklist(BaseModel):
    before: list[str]
    during: list[str]
    after: list[str]


class AdvancedRecommendation(BaseModel):
    title: str
    description: str


class ABTestDesignResult(BaseModel):
    overall_summary: str
    baseline_metrics: list[MetricSnapshot]
    test_ideas: list[TestIdea]
    sample_size_notes: list[str]
    risk_management: list[str]
    execution_checklist: ABExecutionChecklist
    advanced_strategies: list[AdvancedRecommendation]


@dataclass
class ABTestDesignDeps:
    df: pd.DataFrame
    objective: str
    rag_service: Optional[object] = None


class ABTestDesignAgent:
    """A/B æ¸¬è©¦è¨­è¨ˆ Agent"""

    def __init__(self) -> None:
        model_name = os.getenv('OPENAI_MODEL', 'gpt-5-nano')
        self.agent = Agent(
            f'openai:{model_name}',
            output_type=ABTestDesignResult,
            deps_type=ABTestDesignDeps,
            system_prompt=self._get_system_prompt(),
        )
        self._register_tools()

    def _get_system_prompt(self) -> str:
        return """
ä½ æ˜¯ Meta å»£å‘Š A/B æ¸¬è©¦ç­–ç•¥å°ˆå®¶ã€‚è«‹ä¾æ“šè¼¸å…¥è³‡æ–™èˆ‡å·¥å…·æä¾›å®Œæ•´æ¸¬è©¦è¨­è¨ˆå»ºè­°ï¼Œå…§å®¹éœ€ç¬¦åˆ `ABTestDesignResult` çµæ§‹ä¸¦ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
"""

    def _register_tools(self) -> None:
        @self.agent.tool
        def compute_baseline_metrics(ctx: RunContext[ABTestDesignDeps]) -> dict:
            df = ctx.deps.df
            metrics = []
            mapping = {
                'èŠ±è²»é‡‘é¡ (TWD)': 'èŠ±è²» (TWD)',
                'è³¼è²· ROASï¼ˆå»£å‘ŠæŠ•è³‡å ±é…¬çŽ‡ï¼‰': 'å¹³å‡ ROAS',
                'è³¼è²·æ¬¡æ•¸': 'ç¸½è³¼è²·æ•¸',
                'CTRï¼ˆå…¨éƒ¨ï¼‰': 'å¹³å‡ CTR (%)',
                'é€£çµé»žæ“Šæ¬¡æ•¸': 'é€£çµé»žæ“Šæ•¸'
            }
            for col, name in mapping.items():
                if col in df:
                    value = df[col].mean() if 'å¹³å‡' in name else df[col].sum()
                    if col == 'CTRï¼ˆå…¨éƒ¨ï¼‰':
                        value *= 100
                    metrics.append({'name': name, 'value': float(value)})
            return {'baseline_metrics': metrics}

        @self.agent.tool
        def detect_opportunities(ctx: RunContext[ABTestDesignDeps]) -> dict:
            df = ctx.deps.df
            opportunities = []
            if 'headline' in df and 'è³¼è²· ROASï¼ˆå»£å‘ŠæŠ•è³‡å ±é…¬çŽ‡ï¼‰' in df:
                grouped = df.groupby('headline', as_index=False).agg({
                    'è³¼è²· ROASï¼ˆå»£å‘ŠæŠ•è³‡å ±é…¬çŽ‡ï¼‰': 'mean',
                    'é€£çµé»žæ“Šæ¬¡æ•¸': 'sum'
                })
                top = grouped[grouped['é€£çµé»žæ“Šæ¬¡æ•¸'] >= 50]
                if not top.empty:
                    best = top.sort_values('è³¼è²· ROASï¼ˆå»£å‘ŠæŠ•è³‡å ±é…¬çŽ‡ï¼‰', ascending=False).head(3)
                    worst = top.sort_values('è³¼è²· ROASï¼ˆå»£å‘ŠæŠ•è³‡å ±é…¬çŽ‡ï¼‰', ascending=True).head(3)
                    opportunities.append({
                        'type': 'headline',
                        'best': best.to_dict('records'),
                        'worst': worst.to_dict('records'),
                    })
            if 'call_to_action_type' in df and 'CTRï¼ˆå…¨éƒ¨ï¼‰' in df:
                grouped = df.groupby('call_to_action_type', as_index=False)['CTRï¼ˆå…¨éƒ¨ï¼‰'].mean()
                opportunities.append({
                    'type': 'cta',
                    'cta_performance': grouped.to_dict('records')
                })
            return {'opportunities': opportunities}

        @self.agent.tool
        def load_rag_examples(ctx: RunContext[ABTestDesignDeps]) -> dict:
            rag = ctx.deps.rag_service
            if not rag:
                return {'available': False}
            try:
                docs = rag.search_similar_ads('A/B æ¸¬è©¦ æˆåŠŸ æ¡ˆä¾‹', top_k=3)
                results = []
                for doc in docs:
                    results.append({
                        'content': getattr(doc, 'page_content', ''),
                        'metadata': getattr(doc, 'metadata', {})
                    })
                return {'available': True, 'examples': results}
            except Exception:
                return {'available': False}

    async def design(
        self,
        df: pd.DataFrame,
        objective: str,
        rag_service: Optional[object] = None,
    ) -> ABTestDesignResult:
        deps = ABTestDesignDeps(df=df, objective=objective, rag_service=rag_service)
        prompt = f"""
è«‹æ ¹æ“šå·¥å…·æä¾›çš„è³‡æ–™èˆ‡æ¸¬è©¦ç›®æ¨™ã€Œ{objective}ã€è¨­è¨ˆå®Œæ•´ A/B æ¸¬è©¦è¨ˆç•«ã€‚
éœ€è¦åŒ…å«ï¼šæ‘˜è¦ã€æ¸¬è©¦æƒ³æ³•ã€æ¨£æœ¬èˆ‡é¢¨éšªç®¡ç†ã€åŸ·è¡Œæª¢æŸ¥æ¸…å–®ã€é€²éšŽå»ºè­°ã€‚
"""
        result = await self.agent.run(prompt, deps=deps)
        return result.output

    def design_sync(
        self,
        df: pd.DataFrame,
        objective: str,
        rag_service: Optional[object] = None,
    ) -> ABTestDesignResult:
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        return loop.run_until_complete(self.design(df=df, objective=objective, rag_service=rag_service))
